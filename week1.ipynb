{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To AI - DS2020\n",
    "### Dr. Narayan C Krishnan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week - 01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 01 - 07.01.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-0-2-4 : Credit Course\n",
    "\n",
    "[Course info](https://seekayan.github.io/s25ds2020.html) can be found here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 weeks.... 5 labs(3,6,9,13,,15)... \n",
    "- week 1 (3) - Intelligent and Problem Solving Agents\n",
    "- week 2 (1.5) - Search I\n",
    "- week 3 (3) - Search II, lab 1\n",
    "- week 4 (3) - Adversarial Search\n",
    "- week 5 (3) - Logical Agents and Propositional Logic\n",
    "- week 6 (3) - First Order Logic,, lab 2\n",
    "- week 7 (3) - Buffer week, test 1,\n",
    "- week 8 (3) - Planning\n",
    "- week 9 (3) - Probabilistic Reasoning I, lab 3\n",
    "- week 10 (3) - Probabilistic Reasoning II\n",
    "- week 11 (3) - Decision Theory\n",
    "- week 12 (3) - Markov Decision Process I, test 2\n",
    "- week 13 (3) - Markov Decision Process II lab 4\n",
    "- week 14 (1.5) - Reinforcement Learning I\n",
    "- week 15 (3) - Reinforcement Learning II, lab 5\n",
    "- week 16 (1.5) - Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests - 30% (2 quizzes 15+15) \\\n",
    "Labs - 25% (5*5 and all labs are take home?!) \\\n",
    "End Sem - 45% \\\n",
    "Attendance - 1%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refrence Materials - Aima, [aima website](http://aima.cs.berkeley.edu/global-index.html) \\\n",
    "All class materials will be accessible on moodle... \\\n",
    "And no lec slides except for the first week... Only written notes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intelligent Agent - acts rationally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "     Environment \n",
    "Perce-|       | Action  \n",
    "ption |       |\n",
    "      > Agent >\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "agent(state, environment):\n",
    "      agent.sensors\n",
    "      return action\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rational agent - what is right is defined by a performance measure and we maximize the expected value of the performance measure given the percept sequence to date nad prior knowledge\n",
    "\n",
    "PEAS (performance, environment, actuators, sensors) - specifying the task environment...\n",
    "\n",
    "Types of agents:\n",
    "- simple reflex agents\n",
    "- model based reflex agents\n",
    "- goal based agents\n",
    "- utility based agents\n",
    "- learning agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day 2 - 09.01.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Reflex Agent\n",
    "```\n",
    "function SIMPLE-REFLEX-AGENT(percept) returns an action\n",
    "    persistent: rules, a set of condition–action rules\n",
    "    state ← INTERPRET-INPUT(percept)\n",
    "    rule ← RULE-MATCH(state, rules)\n",
    "    action ← rule.ACTION\n",
    "    return action\n",
    "```\n",
    "\n",
    "There are some condition action rules... \\\n",
    "And it almost instantaneoulsy performs the action... \\\n",
    "The issue is that we have to figure out all the possible conditions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Based Reflex Agent\n",
    "\n",
    "```\n",
    "function MODEL-BASED-REFLEX-AGENT(percept) returns an action\n",
    "    persistent: state, the agent’s current conception of the world state\n",
    "        transition model, a description of how the next state depends on\n",
    "        the current state and action\n",
    "        sensor model, a description of how the current world state is reflected\n",
    "        in the agent’s percepts\n",
    "        rules, a set of condition–action rules\n",
    "        action, the most recent action, initially none\n",
    "    state ← UPDATE-STATE(state, action, percept, transition model, sensor model)\n",
    "    rule ← RULE-MATCH(state, rules)\n",
    "    action ← rule.ACTION\n",
    "    return action\n",
    "```\n",
    "\n",
    "It posses the knowledge of how its action changes the env and how the env is changing it percepts the and estimates the state... \\\n",
    "i.e., the percepts need not provide the complete info of the environment... \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal Based Agents\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "We only specify the goal it needs to do and nothing else... \\\n",
    "It still has the state estimation and knows the actions it takes and its consequences... \\\n",
    "It internally thinks about its actions and plans... In a model based agent we provide the sequence of actions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Based Agents\n",
    "\n",
    "Rather than abstractly defining the goal, we give a utility functions, which gives an idea of how it percieves a particular state (like how happy we are with its action), we try to max out the utility value..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Based Agent\n",
    "\n",
    "We leave it to the agent to figure out what is good and bad... \\\n",
    "The agent has the mentality of rewards and penalties... We'll talk about rl based agents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Solving Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanaiton of a kind: \\\n",
    "Consider a weighted graph and the agent wants to move around ant graph is not a fully connected graph... \\\n",
    "Formulation actions for tihs kind of a world is relativly simpler... \\\n",
    "The problem formulation is important, i.e., we try to identify the states initially \\\n",
    "Then we need to figure out what we need to specify, i.e., from each state we can define what actions can be peeformed from a paticular state... \n",
    "\n",
    "We make some assumptions to do this:\n",
    "- the task environment observable, i.e, it doesn't need any state estimation processes\n",
    "- it is a discrete environment\n",
    "- We assume that the actions are deterministic\n",
    "- The env is static\n",
    "- The actions are sequential\n",
    "- There is a single agent\n",
    "The agent basically is figuring out a seq of actions to go from start state to goal state, this is the search procedure here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem formulation:\n",
    "- Initial state... the agemt begins with an initial state\n",
    "- Actions (ex: D $\\rightarrow$ A; D $\\rightarrow$ B; D $\\rightarrow$ E)\n",
    "- Goal state\n",
    "- Path cost : cost associated with the action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the 8 puzzle problem... we can consider it as vector/list of length 9 containing the elements and we try to figure out stuff... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are essentially doing is a serach...\n",
    "\n",
    "We can't store all the edges and vertices...\\\n",
    "So we have a child generator function \\\n",
    "CGF(D,a) = A \\\n",
    "CGF(D,e) = E \\\n",
    "CGF(D) = {A,E} \\\n",
    "the cgf function either give the next state for a given action or the next possible states given only a particular state \\\n",
    "This gives a tree like structure... \\\n",
    "Frontier list, explored list... \\\n",
    "The search strategies differ in the order of the nodes explored... \\\n",
    "Now this calls for a way to evaluate a search strategy....\n",
    "\n",
    "Some of the evaluations\n",
    "- completeness: if a solution exists, does it always find the solution\n",
    "- optimality: comes only if there's a cost associated with actions... i.e., we need to get the least cost solution...\n",
    "- space complexity: memory reqd to conduct the search...\n",
    "- time complexity: time reqd to formulate the solution... \\\n",
    "  Let us see some of the factors to calc the complexities\n",
    "    - b - maximum branching factor (max no of states that can be visited from a state)\n",
    "    - d - shallowest depth of the least cost solution \n",
    "    - m - maximum depth of the search space\n",
    "\n",
    "The solution we are talking about right now is uninformed search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Informed Search (Heuristic Search)\n",
    "\n",
    "Informed search algorithms, also known as heuristic search algorithms, make use of additional problem-specific knowledge to find solutions more efficiently than uninformed search algorithms (such as Breadth-First Search and Depth-First Search). The key idea behind informed search is to use **heuristics** to guide the search towards the most promising paths.\n",
    "\n",
    "#### Key Concepts:\n",
    "1. **Heuristic (h(n))**:\n",
    "   - A heuristic is a function that estimates the cost or distance from a given node `n` to the goal.\n",
    "   - The heuristic function depends only on the current state at node `n`.\n",
    "   - If the node is a goal, then `h(n) = 0` because no further steps are needed.\n",
    "   - A good heuristic helps guide the search towards the goal quickly, reducing the time complexity of the search.\n",
    "   \n",
    "2. **g(n)**:\n",
    "   - This represents the cost of reaching node `n` from the initial state.\n",
    "   \n",
    "3. **f(n)**:\n",
    "   - The evaluation function, often used in algorithms like A*, is the sum of two components: the cost to reach the node and the estimated cost to reach the goal from that node.\n",
    "   - The function `f(n)` is defined as:\n",
    "     \\[\n",
    "     f(n) = g(n) + h(n)\n",
    "     \\]\n",
    "   - The value of `f(n)` helps in prioritizing which node to explore next during the search.\n",
    "\n",
    "4. **Goal**:\n",
    "   - A goal is a state that satisfies the criteria for a solution to the problem.\n",
    "   - When the search reaches a goal node, the solution has been found.\n",
    "\n",
    "### Related Algorithms\n",
    "\n",
    "#### 1. **Greedy Best-First Search (Greedy BFS)**:\n",
    "   - This is a simple heuristic search algorithm that focuses solely on minimizing the estimated cost from the current state to the goal. The evaluation function in Greedy BFS is:\n",
    "     \\[\n",
    "     f(n) = h(n)\n",
    "     \\]\n",
    "   - It chooses the node with the lowest heuristic value (i.e., the node that appears to be closest to the goal) and expands it first. \n",
    "   - **Pros**: Greedy BFS is generally fast because it focuses on the heuristic.\n",
    "   - **Cons**: It doesn't always find the shortest path because it ignores the cost to reach the current node (g(n)).\n",
    "\n",
    "#### 2. __A* Search__:\n",
    "   - A* is a more sophisticated search algorithm that combines the strengths of **Dijkstra’s Algorithm** (which minimizes the cost to reach a node, `g(n)`) and **Greedy Best-First Search** (which minimizes the estimated cost to the goal, `h(n)`).\n",
    "   - The evaluation function in A* is:\n",
    "     \\[\n",
    "     f(n) = g(n) + h(n)\n",
    "     \\]\n",
    "   - **A* Search** always finds the optimal solution if the heuristic is admissible (i.e., it never overestimates the true cost to reach the goal).\n",
    "   - **Pros**: A* is complete, optimal (with an admissible heuristic), and can be more efficient than uninformed search methods.\n",
    "   - **Cons**: The performance of A* can be affected by the quality of the heuristic.\n",
    "\n",
    "### Summary of Algorithms:\n",
    "1. **Greedy Best-First Search**:\n",
    "   - **Evaluation function**: `f(n) = h(n)`\n",
    "   - Focus: Minimizes the estimated cost to the goal.\n",
    "   - Not guaranteed to find the shortest path.\n",
    "\n",
    "2. __A* Search__:\n",
    "   - **Evaluation function**: `f(n) = g(n) + h(n)`\n",
    "   - Focus: Minimizes the total cost (path cost + estimated cost to the goal).\n",
    "   - Guaranteed to find the optimal solution if the heuristic is admissible.\n",
    "\n",
    "In conclusion, **informed search** uses heuristics to improve the efficiency of the search process. Both Greedy BFS and A* search are popular informed search algorithms, with A* being more commonly used for optimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a Python implementation of **Greedy Best-First Search (Greedy BFS)** and **A* Search**. Both algorithms require a problem space where the nodes, state transitions, and heuristics are defined. For simplicity, I'll implement these searches on a grid-based problem where the goal is to move from a start position to a goal position.\n",
    "\n",
    "### Python Implementation:\n",
    "\n",
    "#### 1. **Greedy Best-First Search (Greedy BFS)**:\n",
    "\n",
    "\n",
    "\n",
    "#### 2. __A* Search__:\n",
    "\n",
    "\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "#### Greedy Best-First Search:\n",
    "- **open_list**: This is a priority queue (using `heapq` in Python) that stores nodes to explore, with each node having a priority determined by its heuristic value (`h(n)`).\n",
    "- **closed_list**: A set that keeps track of nodes that have already been explored.\n",
    "- **heuristic function**: In this example, the Manhattan distance is used as the heuristic function (`manhattan_heuristic`), which estimates the cost from the current node to the goal.\n",
    "- The algorithm explores nodes by always selecting the one with the lowest heuristic value (`f(n) = h(n)`).\n",
    "\n",
    "#### A* Search:\n",
    "- **open_list**: Similar to Greedy BFS, this is a priority queue, but in A*, nodes are prioritized by the total cost function `f(n) = g(n) + h(n)` where `g(n)` is the cost to reach node `n` from the start and `h(n)` is the heuristic estimate to the goal.\n",
    "- **g_cost**: A dictionary that stores the cost to reach each node from the start.\n",
    "- The algorithm explores nodes by considering both the cost to reach them (`g(n)`) and the estimated cost to the goal (`h(n)`).\n",
    "\n",
    "### Example Grid:\n",
    "- In both algorithms, we assume the grid allows movement in four directions (up, down, left, right).\n",
    "- The heuristic used (`manhattan_heuristic`) is a simple estimation for a grid where diagonal movement is not allowed.\n",
    "\n",
    "### Results:\n",
    "- **Greedy BFS** will find a path to the goal based on the heuristic alone, but it doesn't guarantee the shortest path.\n",
    "- **A* Search** will also find a path, but it guarantees the shortest path if the heuristic is admissible.\n",
    "\n",
    "These algorithms can be adapted for more complex environments or different types of problems by modifying the neighbor generation and the heuristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# Greedy Best-First Search\n",
    "def greedy_bfs(start, goal, heuristic):\n",
    "    open_list = []  # Priority queue to store nodes\n",
    "    closed_list = set()  # Set of explored nodes\n",
    "    \n",
    "    # Push the start node into the priority queue with its heuristic value\n",
    "    heapq.heappush(open_list, (heuristic(start, goal), start))\n",
    "    \n",
    "    # While there are nodes to explore\n",
    "    while open_list:\n",
    "        # Pop the node with the lowest heuristic value\n",
    "        _, current = heapq.heappop(open_list)\n",
    "        \n",
    "        # If the current node is the goal, we found the solution\n",
    "        if current == goal:\n",
    "            return True\n",
    "        \n",
    "        closed_list.add(current)\n",
    "        \n",
    "        # Explore the neighbors of the current node\n",
    "        for neighbor in get_neighbors(current):\n",
    "            if neighbor not in closed_list:\n",
    "                heapq.heappush(open_list, (heuristic(neighbor, goal), neighbor))\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Example of a simple Manhattan distance heuristic\n",
    "def manhattan_heuristic(current, goal):\n",
    "    return abs(current[0] - goal[0]) + abs(current[1] - goal[1])\n",
    "\n",
    "# Example function to get neighbors of a node\n",
    "def get_neighbors(node):\n",
    "    x, y = node\n",
    "    neighbors = [(x + 1, y), (x - 1, y), (x, y + 1), (x, y - 1)]  # 4-connected grid\n",
    "    return neighbors\n",
    "\n",
    "# Example Usage\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "\n",
    "# Run Greedy BFS\n",
    "found = greedy_bfs(start, goal, manhattan_heuristic)\n",
    "print(\"Found path:\", found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "# A* Search\n",
    "def a_star_search(start, goal, heuristic):\n",
    "    open_list = []  # Priority queue to store nodes\n",
    "    closed_list = set()  # Set of explored nodes\n",
    "    g_cost = {start: 0}  # Cost to reach a node from the start\n",
    "    \n",
    "    # Push the start node into the priority queue with f(n) = g(n) + h(n)\n",
    "    heapq.heappush(open_list, (heuristic(start, goal), start))\n",
    "    \n",
    "    # While there are nodes to explore\n",
    "    while open_list:\n",
    "        _, current = heapq.heappop(open_list)\n",
    "        \n",
    "        # If the current node is the goal, we found the solution\n",
    "        if current == goal:\n",
    "            return True\n",
    "        \n",
    "        closed_list.add(current)\n",
    "        \n",
    "        # Explore the neighbors of the current node\n",
    "        for neighbor in get_neighbors(current):\n",
    "            if neighbor not in closed_list:\n",
    "                tentative_g = g_cost[current] + 1  # Assuming uniform cost for neighbors\n",
    "                \n",
    "                # If this path to the neighbor is better, add it to the open list\n",
    "                if neighbor not in g_cost or tentative_g < g_cost[neighbor]:\n",
    "                    g_cost[neighbor] = tentative_g\n",
    "                    f_cost = tentative_g + heuristic(neighbor, goal)\n",
    "                    heapq.heappush(open_list, (f_cost, neighbor))\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Example Usage\n",
    "start = (0, 0)\n",
    "goal = (3, 3)\n",
    "\n",
    "# Run A* Search\n",
    "found = a_star_search(start, goal, manhattan_heuristic)\n",
    "print(\"Found path:\", found)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
